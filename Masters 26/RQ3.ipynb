{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c973a4de",
   "metadata": {},
   "source": [
    "## **RQ3**\n",
    "\n",
    "> _Em modelos globais para previsÃ£o multi-horizonte de poluentes, quando variÃ¡veis exÃ³genas (meteorologia + co-poluentes) geram ganho real de desempenho â€” e esse ganho Ã© explicÃ¡vel por atribuiÃ§Ãµes consistentes e fisicamente plausÃ­veis?_\n",
    "\n",
    "* AvaliaÃ§Ã£o empÃ­rica (nÃ£o arquitetural) do impacto de exÃ³genas;\n",
    "* Horizontes de 7, 14 e 30 dias;\n",
    "* AnÃ¡lise explÃ­cita de:\n",
    "   * quando exÃ³genas ajudam,\n",
    "   * quando nÃ£o ajudam,\n",
    "   * possÃ­veis razÃµes fÃ­sicas e estatÃ­sticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b7db2",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba11a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gustavo.filho\\Documents\\Python\\Masters_New\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'\n",
    "\n",
    "import optuna\n",
    "import itertools\n",
    "import shutil\n",
    "import time\n",
    "import functools\n",
    "import gc\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.subplots\n",
    "import plotly.io as pio\n",
    "from graphmodex import plotlymodex\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab499e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-03-01 20:09:02,312\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2026-03-01 20:09:02,740\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "ðŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n"
     ]
    }
   ],
   "source": [
    "import neuralforecast\n",
    "import mlforecast\n",
    "import statsforecast\n",
    "import utilsforecast\n",
    "import coreforecast\n",
    "\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import (\n",
    "    Naive, SeasonalNaive,\n",
    "    AutoARIMA, AutoCES, AutoETS, AutoTheta,\n",
    ")\n",
    "\n",
    "from mlforecast import MLForecast\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mlforecast.lag_transforms import ExpandingMean, RollingMean\n",
    "from mlforecast.target_transforms import Differences\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import (\n",
    "    NBEATS, NHITS, NBEATSx,\n",
    "    GRU, Informer, LSTM\n",
    ")\n",
    "from neuralforecast.losses.pytorch import MSE, SMAPE, MAE\n",
    "\n",
    "from mlforecast.utils import PredictionIntervals\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(\n",
    "    max_steps=4,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False,\n",
    "    enable_model_summary=False\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# REPRODUCTIBILITY\n",
    "# ==================================================\n",
    "import random\n",
    "import torch\n",
    "\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1ce0c",
   "metadata": {},
   "source": [
    "### Results Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0e9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_RESULTS = Path(\"Results/RQ3\")\n",
    "BASE_RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_results(df, model_family, pollutant, horizon_label, fold=None):\n",
    "\n",
    "    out_dir = BASE_RESULTS / model_family\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if fold is not None:\n",
    "        fname = f\"{pollutant}_{fold}_{horizon_label}.csv\"\n",
    "    else:\n",
    "        fname = f\"{pollutant}_{horizon_label}.csv\"\n",
    "\n",
    "    df.to_csv(out_dir / fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05ef21",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8408a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# DATA\n",
    "# ===============================\n",
    "df = pd.read_parquet(r'..\\Data\\CAMS\\processed\\eac4_era5_2010_2024_brasil_enhanced.parquet')\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# POLLUTANTS\n",
    "# ===============================\n",
    "pm2p5 = (\n",
    "    df\n",
    "    .copy()\n",
    "    .rename(columns={\n",
    "        'pm2p5': 'y',\n",
    "        'valid_time': 'ds'        \n",
    "    })\n",
    ")\n",
    "\n",
    "go3 = (\n",
    "    df\n",
    "    .copy()\n",
    "    .rename(columns={\n",
    "        'go3': 'y',\n",
    "        'valid_time': 'ds'        \n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5c5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_COLS = [ \"ds\", \"unique_id\", \"y\"]\n",
    "\n",
    "O3_F1 = [ \"no2\", \"co\"]\n",
    "O3_F2 = [ \"u10\", \"v10\", \"wind_speed\", \"t2m\", \"d2m\", \"RH\", \"ssrd_flux\", \"blh\"]\n",
    "O3_F3 = O3_F1 + O3_F2\n",
    "\n",
    "PM_F1 = [ \"so2\", \"no2\", \"co\"]\n",
    "PM_F2 = [\"u10\", \"v10\", \"wind_speed\", \"t2m\", \"d2m\", \"RH\", \"ssrd_flux\", \"blh\"]\n",
    "PM_F3 = PM_F1 + PM_F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f49b5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "pollutant_dict = {\n",
    "    'go3': {\n",
    "        'f0': {\n",
    "            'df': go3[BASE_COLS].copy(),\n",
    "            'exog_cols': []\n",
    "        },\n",
    "        'f1': {\n",
    "            'df': go3[BASE_COLS + O3_F1].copy(),\n",
    "            'exog_cols': O3_F1\n",
    "        },\n",
    "        'f2': {\n",
    "            'df': go3[BASE_COLS + O3_F2].copy(),\n",
    "            'exog_cols': O3_F2\n",
    "        },\n",
    "        'f3': {\n",
    "            'df': go3[BASE_COLS + O3_F3].copy(),\n",
    "            'exog_cols': O3_F3\n",
    "        },\n",
    "        'scaler': 1e8\n",
    "    },\n",
    "\n",
    "    'pm2p5': {\n",
    "        'f0': {\n",
    "            'df': pm2p5[BASE_COLS].copy(),\n",
    "            'exog_cols': []\n",
    "        },\n",
    "        'f1': {\n",
    "            'df': pm2p5[BASE_COLS + PM_F1].copy(),\n",
    "            'exog_cols': PM_F1\n",
    "        },\n",
    "        'f2': {\n",
    "            'df': pm2p5[BASE_COLS + PM_F2].copy(),\n",
    "            'exog_cols': PM_F2\n",
    "        },\n",
    "        'f3': {\n",
    "            'df': pm2p5[BASE_COLS + PM_F3].copy(),\n",
    "            'exog_cols': PM_F3\n",
    "        },\n",
    "        'scaler': 1e9\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1c5ea",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a26aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CONFIG\n",
    "# ===============================\n",
    "steps_per_day = 8\n",
    "two_year_steps = 2 * 365 * steps_per_day\n",
    "target_windows = 30\n",
    "\n",
    "FREQ = '3h'\n",
    "SEASON_LENGTH = 8 \n",
    "\n",
    "experiments_dict = {\n",
    "    # '1 days': {\n",
    "    #     'horizon': 8*1,\n",
    "    #     'step_size': max(8*1, two_year_steps // target_windows),\n",
    "    #     'windows': target_windows,\n",
    "    # },\n",
    "    '7 days': {\n",
    "        'horizon': 8*7,\n",
    "        'step_size': max(8*7, two_year_steps // target_windows),\n",
    "        'windows': target_windows,\n",
    "    },\n",
    "    '14 days': {\n",
    "        'horizon': 8*14,\n",
    "        'step_size': max(8*14, two_year_steps // target_windows),\n",
    "        'windows': target_windows,\n",
    "    },\n",
    "    '30 days': {\n",
    "        'horizon': 8*30,\n",
    "        'step_size': 8*30,\n",
    "        'windows': two_year_steps // (8*30),  # 24\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e2b15",
   "metadata": {},
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eec9d3b",
   "metadata": {},
   "source": [
    "### Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb9740",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollutant_name, pollutant_ in pollutant_dict.items():\n",
    "    \n",
    "    scaler = pollutant_['scaler']\n",
    "    \n",
    "    for fk_label in ['f0', 'f1', 'f2', 'f3']:\n",
    "        \n",
    "        for horizon_label, experiment_ in experiments_dict.items():\n",
    "            \n",
    "            try:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Running RQ3 | {pollutant_name} | {fk_label} | {horizon_label}\")\n",
    "\n",
    "                # --------------------------------\n",
    "                # DATA (copy fresh every iteration)\n",
    "                # --------------------------------\n",
    "                df_config = pollutant_[fk_label]['df'].copy()\n",
    "                exog_cols = pollutant_[fk_label]['exog_cols']\n",
    "\n",
    "                # Scale target\n",
    "                df_config['y'] = df_config['y'] * scaler\n",
    "\n",
    "                # --------------------------------\n",
    "                # MODELS\n",
    "                # --------------------------------\n",
    "                models = [\n",
    "                    GRU(\n",
    "                        h=experiment_['horizon'],\n",
    "                        input_size=14*8,\n",
    "                        max_steps=1000,\n",
    "                        learning_rate=1e-3,\n",
    "                        batch_size=32,\n",
    "                        windows_batch_size=1024,\n",
    "                        random_seed=SEED,\n",
    "                        alias='GRU',\n",
    "                        loss=MAE(),\n",
    "                        hist_exog_list=exog_cols if exog_cols else None,\n",
    "                        logger=False,\n",
    "                    ),\n",
    "                    LSTM(\n",
    "                        h=experiment_['horizon'],\n",
    "                        input_size=14*8,\n",
    "                        max_steps=1000,\n",
    "                        learning_rate=1e-3,\n",
    "                        batch_size=32,\n",
    "                        windows_batch_size=1024,\n",
    "                        random_seed=SEED,\n",
    "                        alias='LSTM',\n",
    "                        loss=MAE(),\n",
    "                        hist_exog_list=exog_cols if exog_cols else None,\n",
    "                        logger=False,\n",
    "                    ),\n",
    "                    NBEATSx(\n",
    "                        h=experiment_['horizon'],\n",
    "                        input_size=14*8,\n",
    "                        stack_types=[\"identity\", \"trend\", \"seasonality\"],\n",
    "                        n_blocks=[1, 1, 1],\n",
    "                        mlp_units=3 * [[256, 256]],\n",
    "                        shared_weights=True,\n",
    "                        activation='ReLU',\n",
    "                        max_steps=1000,\n",
    "                        learning_rate=1e-3,\n",
    "                        batch_size=32,\n",
    "                        windows_batch_size=1024,\n",
    "                        random_seed=SEED,\n",
    "                        alias='NBEATSx-I',\n",
    "                        hist_exog_list=exog_cols if exog_cols else None,\n",
    "                        loss=MAE(),\n",
    "                    ),\n",
    "                    NBEATSx(\n",
    "                        h=experiment_['horizon'],\n",
    "                        input_size=14*8,\n",
    "                        stack_types=['identity'] * 3,\n",
    "                        n_blocks=[2, 2, 2],\n",
    "                        mlp_units=3 * [[256, 256]],\n",
    "                        shared_weights=False,\n",
    "                        activation='ReLU',\n",
    "                        max_steps=1000,\n",
    "                        learning_rate=1e-3,\n",
    "                        batch_size=32,\n",
    "                        windows_batch_size=1024,\n",
    "                        random_seed=SEED,\n",
    "                        alias='NBEATSx-G',\n",
    "                        logger=False,\n",
    "                        hist_exog_list=exog_cols if exog_cols else None,\n",
    "                        loss=MAE(),\n",
    "                    ),\n",
    "                    NHITS(\n",
    "                        h=experiment_['horizon'],\n",
    "                        input_size=14*8,\n",
    "                        n_blocks=[1, 1, 1],\n",
    "                        mlp_units=3 * [[256, 256]],\n",
    "                        n_pool_kernel_size=[2, 2, 1],\n",
    "                        n_freq_downsample=[4, 2, 1],\n",
    "                        activation='ReLU',\n",
    "                        dropout_prob_theta=0.1,\n",
    "                        max_steps=1000,\n",
    "                        learning_rate=1e-3,\n",
    "                        batch_size=32,\n",
    "                        windows_batch_size=1024,\n",
    "                        random_seed=SEED,\n",
    "                        alias='NHITS',\n",
    "                        logger=False,\n",
    "                        hist_exog_list=exog_cols if exog_cols else None,\n",
    "                        loss=MAE(),\n",
    "                    ),\n",
    "                ]\n",
    "\n",
    "                nf = NeuralForecast(models=models, freq=FREQ)\n",
    "\n",
    "                # --------------------------------\n",
    "                # CROSS VALIDATION\n",
    "                # --------------------------------\n",
    "                start_time = time.time()\n",
    "\n",
    "                results_nf = nf.cross_validation(\n",
    "                    df=df_config,\n",
    "                    h=experiment_['horizon'],\n",
    "                    n_windows=experiment_['windows'],\n",
    "                    step_size=experiment_['step_size'],\n",
    "                    refit=False,\n",
    "                )\n",
    "\n",
    "                end_time = time.time()\n",
    "                total_time = end_time - start_time\n",
    "\n",
    "                results_nf['fit_time_seconds'] = total_time\n",
    "\n",
    "                # --------------------------------\n",
    "                # SCALE BACK\n",
    "                # --------------------------------\n",
    "                for col in ['y', 'GRU', 'LSTM', 'NBEATSx-I', 'NBEATSx-G', 'NHITS']:\n",
    "                    if col in results_nf.columns:\n",
    "                        results_nf[col] = results_nf[col] / scaler\n",
    "\n",
    "                # --------------------------------\n",
    "                # SAVE\n",
    "                # --------------------------------\n",
    "                save_results(\n",
    "                    results_nf,\n",
    "                    model_family=\"dl\",\n",
    "                    pollutant=pollutant_name,\n",
    "                    horizon_label=horizon_label.replace(\" \", \"\"),\n",
    "                    fold=fk_label\n",
    "                )\n",
    "\n",
    "                # --------------------------------\n",
    "                # CLEAN MEMORY\n",
    "                # --------------------------------\n",
    "                del nf\n",
    "                del models\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed on {pollutant_name} {fk_label} {horizon_label}: {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596c623",
   "metadata": {},
   "source": [
    "### Machine Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4adb1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RQ3-ML | pm2p5 | f3 | 30 days\n",
      "Failed on pm2p5 f3 30 days: Unable to allocate 96.3 MiB for an array with shape (1, 12623616) and data type int64\n"
     ]
    }
   ],
   "source": [
    "for pollutant_name, pollutant_ in pollutant_dict.items():\n",
    "    \n",
    "    scaler = pollutant_['scaler']\n",
    "    \n",
    "    for fk_label in ['f0', 'f1', 'f2', 'f3']:\n",
    "        \n",
    "        for horizon_label, experiment_ in experiments_dict.items():\n",
    "            \n",
    "            try:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Running RQ3-ML | {pollutant_name} | {fk_label} | {horizon_label}\")\n",
    "\n",
    "                # --------------------------------\n",
    "                # DATA (copy fresh every iteration)\n",
    "                # --------------------------------\n",
    "                df_config = pollutant_[fk_label]['df']\n",
    "                df_config = df_config.sort_values(['unique_id', 'ds'])\n",
    "                \n",
    "                # --------------------------------\n",
    "                # MODEL (LightGBM only)\n",
    "                # --------------------------------\n",
    "                lgb_model = lgb.LGBMRegressor(\n",
    "                    n_estimators=500,\n",
    "                    learning_rate=0.05,\n",
    "                    num_leaves=31,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=SEED,\n",
    "                    n_jobs=10,\n",
    "                )\n",
    "\n",
    "                # --------------------------------\n",
    "                # MLForecast\n",
    "                # --------------------------------\n",
    "                mlf = MLForecast(\n",
    "                    models={'LightGBM': lgb_model},\n",
    "                    freq=FREQ,\n",
    "                    lags=[1, 2, 4, 8, 16, 24, 56, 112],\n",
    "                    lag_transforms={\n",
    "                        8:  [RollingMean(window_size=8), ExpandingMean()],\n",
    "                        56: [RollingMean(window_size=56)],\n",
    "                    },\n",
    "                    date_features=[\n",
    "                        'hour',\n",
    "                        'dayofweek',\n",
    "                        'month',\n",
    "                        'dayofyear',\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                # --------------------------------\n",
    "                # CROSS VALIDATION\n",
    "                # --------------------------------\n",
    "                start_time = time.time()\n",
    "\n",
    "                results_ml = mlf.cross_validation(\n",
    "                    df=df_config,\n",
    "                    h=experiment_['horizon'],\n",
    "                    n_windows=experiment_['windows'],\n",
    "                    step_size=experiment_['step_size'],\n",
    "                    refit=False,\n",
    "                )\n",
    "\n",
    "                end_time = time.time()\n",
    "                total_time = end_time - start_time\n",
    "\n",
    "                results_ml['fit_time_seconds'] = total_time\n",
    "\n",
    "                # --------------------------------\n",
    "                # SAVE\n",
    "                # --------------------------------\n",
    "                save_results(\n",
    "                    results_ml,\n",
    "                    model_family=\"ml\",\n",
    "                    pollutant=pollutant_name,\n",
    "                    horizon_label=horizon_label.replace(\" \", \"\"),\n",
    "                    fold=fk_label\n",
    "                )\n",
    "\n",
    "                # --------------------------------\n",
    "                # CLEAN MEMORY\n",
    "                # --------------------------------\n",
    "                del mlf\n",
    "                del lgb_model\n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed on {pollutant_name} {fk_label} {horizon_label}: {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae113ef9",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db0145a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FULL | go3 | sudeste | 7days\n",
      "Building FULL | go3 | sudeste | 30days\n",
      "Building FULL | go3 | sul | 7days\n",
      "Building FULL | go3 | sul | 30days\n",
      "Building FULL | go3 | centro_oeste | 7days\n",
      "Building FULL | go3 | centro_oeste | 30days\n",
      "Building FULL | go3 | coastal | 7days\n",
      "Building FULL | go3 | coastal | 30days\n",
      "Building FULL | pm2p5 | sudeste | 7days\n",
      "Building FULL | pm2p5 | sudeste | 30days\n",
      "Building FULL | pm2p5 | sul | 7days\n",
      "Building FULL | pm2p5 | sul | 30days\n",
      "Building FULL | pm2p5 | centro_oeste | 7days\n",
      "Building FULL | pm2p5 | centro_oeste | 30days\n",
      "Building FULL | pm2p5 | coastal | 7days\n",
      "Building FULL | pm2p5 | coastal | 30days\n"
     ]
    }
   ],
   "source": [
    "BASE_RESULTS = Path(\"Results/RQ2\")\n",
    "FULL_DIR = BASE_RESULTS / \"full\"\n",
    "FULL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "families = [\"ml\", \"dl\", \"neural\"]\n",
    "\n",
    "def build_full_results(pollutant, fold, horizon_label):\n",
    "    \"\"\"\n",
    "    Concatena famÃ­lias horizontalmente via concat,\n",
    "    garantindo que nÃ£o haja perda de linhas.\n",
    "    \"\"\"\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for family in families:\n",
    "        fpath = BASE_RESULTS / family / f\"{pollutant}_{fold}_{horizon_label.replace(' ', '')}.csv\"\n",
    "\n",
    "        if fpath.exists():\n",
    "            df = pd.read_csv(fpath)\n",
    "\n",
    "            if \"fit_time_seconds\" in df.columns:\n",
    "                df = df.drop(columns=[\"fit_time_seconds\"])\n",
    "\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            print(f\"âš ï¸ Missing: {fpath}\")\n",
    "\n",
    "    if not dfs:\n",
    "        return None\n",
    "\n",
    "    # Ordenar todos igualmente\n",
    "    for i in range(len(dfs)):\n",
    "        dfs[i] = dfs[i].sort_values(\n",
    "            [\"unique_id\", \"ds\", \"cutoff\", \"fold\"]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    # Usar o primeiro como base\n",
    "    base = dfs[0][[\"unique_id\", \"ds\", \"cutoff\", \"y\", \"fold\"]].copy()\n",
    "\n",
    "    # Adicionar modelos das outras famÃ­lias\n",
    "    for df in dfs:\n",
    "\n",
    "        model_cols = [\n",
    "            c for c in df.columns\n",
    "            if c not in [\"unique_id\", \"ds\", \"cutoff\", \"y\", \"fold\"]\n",
    "        ]\n",
    "\n",
    "        base = pd.concat(\n",
    "            [base, df[model_cols]],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "for pollutant_name, pollutant_dict in pollutant_dict.items():\n",
    "\n",
    "    for fold_name in pollutant_dict.keys():\n",
    "\n",
    "        if fold_name == \"scaler\":\n",
    "            continue\n",
    "\n",
    "        for horizon_label in experiments_dict.keys():\n",
    "\n",
    "            horizon_clean = horizon_label.replace(\" \", \"\")\n",
    "\n",
    "            print(f\"Building FULL | {pollutant_name} | {fold_name} | {horizon_clean}\")\n",
    "\n",
    "            df_full = build_full_results(\n",
    "                pollutant=pollutant_name,\n",
    "                fold=fold_name,\n",
    "                horizon_label=horizon_clean\n",
    "            )\n",
    "\n",
    "            if df_full is None:\n",
    "                continue\n",
    "\n",
    "            df_full.to_csv(\n",
    "                FULL_DIR / f\"{pollutant_name}_{fold_name}_{horizon_clean}.csv\",\n",
    "                index=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bbb64f",
   "metadata": {},
   "source": [
    "## **EstatÃ­sticas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a434eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# METRICS\n",
    "# ==================================================\n",
    "\n",
    "def mae(y, yhat):\n",
    "    return np.mean(np.abs(y - yhat))\n",
    "\n",
    "def mse(y, yhat):\n",
    "    return np.mean((y - yhat) ** 2)\n",
    "\n",
    "def rmse(y, yhat):\n",
    "    return np.sqrt(mse(y, yhat))\n",
    "\n",
    "def smape(y, yhat):\n",
    "    denom = (np.abs(y) + np.abs(yhat)) / 2\n",
    "    mask = denom != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y[mask] - yhat[mask]) / denom[mask])\n",
    "\n",
    "def mae_conditional(y, yhat, threshold):\n",
    "    mask = y >= threshold\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y[mask] - yhat[mask]))\n",
    "\n",
    "def bias_conditional(y, yhat, threshold):\n",
    "    mask = y >= threshold\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(yhat[mask] - y[mask])\n",
    "\n",
    "def skill_score(model_err, baseline_err):\n",
    "    if baseline_err == 0 or np.isnan(baseline_err):\n",
    "        return np.nan\n",
    "    return 1 - model_err / baseline_err\n",
    "\n",
    "def extreme_event_metrics(y, yhat, threshold):\n",
    "    y_event = y >= threshold\n",
    "    yhat_event = yhat >= threshold\n",
    "\n",
    "    tp = np.sum(y_event & yhat_event)\n",
    "    fp = np.sum(~y_event & yhat_event)\n",
    "    fn = np.sum(y_event & ~yhat_event)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "\n",
    "    if precision > 0 and recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        f1 = np.nan\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def mase_denom(y_train, m=1):\n",
    "    \"\"\"\n",
    "    Mean absolute error of seasonal naive (lag m) on in-sample training.\n",
    "    This is the standard denominator of MASE.\n",
    "    \"\"\"\n",
    "    y_train = np.asarray(y_train)\n",
    "    if len(y_train) <= m:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y_train[m:] - y_train[:-m]))\n",
    "\n",
    "def mase(y_true, y_pred, y_train, m=1):\n",
    "    \"\"\"\n",
    "    MASE = MAE(test) / MAE(seasonal naive in-sample, lag m)\n",
    "    \"\"\"\n",
    "    denom = mase_denom(y_train, m=m)\n",
    "    if denom == 0 or np.isnan(denom):\n",
    "        return np.nan\n",
    "    return mae(y_true, y_pred) / denom\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# CONFIG\n",
    "# ==================================================\n",
    "\n",
    "RQ2_BASE = Path(\"Results/RQ2/full\")\n",
    "RQ1_BASE = Path(\"Results/RQ1/full\")\n",
    "\n",
    "baseline_name = \"Naive\"\n",
    "WINDOW_P95 = 365 * 8  # 1 ano em 3h\n",
    "\n",
    "MASE_M = 56  # weekly seasonality for 3-hourly data\n",
    "\n",
    "records = []\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# LOOP RQ2 FILES\n",
    "# ==================================================\n",
    "\n",
    "for file in RQ2_BASE.glob(\"*.csv\"):\n",
    "\n",
    "    # Ex: go3_sudeste_1days.csv\n",
    "    parts = file.stem.split(\"_\")\n",
    "\n",
    "    pollutant = parts[0]\n",
    "    horizon = parts[-1]\n",
    "    fold = \"_\".join(parts[1:-1])\n",
    "\n",
    "    print(f\"Evaluating | {pollutant} | {fold} | {horizon}\")\n",
    "\n",
    "    df_pred = pd.read_csv(file)\n",
    "    df_pred[\"ds\"] = pd.to_datetime(df_pred[\"ds\"])\n",
    "    df_pred[\"cutoff\"] = pd.to_datetime(df_pred[\"cutoff\"])\n",
    "\n",
    "    # --------------------------------------\n",
    "    # GROUND TRUTH (VINDO DO pollutant_dict)\n",
    "    # --------------------------------------\n",
    "    test_df = pollutant_dict[pollutant][fold][\"test_df\"].copy()\n",
    "    test_df[\"ds\"] = pd.to_datetime(test_df[\"ds\"])\n",
    "\n",
    "    # --------------------------------------\n",
    "    # LOAD RQ1 (PODE NÃƒO EXISTIR PARA ALGUNS IDs)\n",
    "    # --------------------------------------\n",
    "    rq1_path = RQ1_BASE / f\"{pollutant}_{horizon}.csv\"\n",
    "\n",
    "    if rq1_path.exists():\n",
    "        df_rq1 = pd.read_csv(rq1_path)\n",
    "        df_rq1[\"ds\"] = pd.to_datetime(df_rq1[\"ds\"])\n",
    "        df_rq1[\"cutoff\"] = pd.to_datetime(df_rq1[\"cutoff\"])\n",
    "\n",
    "        df_pred = df_pred.merge(\n",
    "            df_rq1,\n",
    "            on=[\"unique_id\", \"ds\", \"cutoff\"],\n",
    "            suffixes=(\"\", \"_RQ1\"),\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------\n",
    "    # MODELOS\n",
    "    # --------------------------------------\n",
    "    model_cols = [\n",
    "        c for c in df_pred.columns\n",
    "        if (\n",
    "            c not in [\"unique_id\", \"ds\", \"cutoff\", \"y\", \"fold\"]\n",
    "            and not c.endswith(\"_RQ1\")\n",
    "            and df_pred[c].dtype in [np.float64, np.float32]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # --------------------------------------\n",
    "    # LOOP POR UNIQUE_ID + CUTOFF\n",
    "    # --------------------------------------\n",
    "    for (uid, cutoff), df_fold in df_pred.groupby([\"unique_id\", \"cutoff\"]):\n",
    "\n",
    "        y_train_full = (\n",
    "            test_df\n",
    "            .query(\"unique_id == @uid and ds <= @cutoff\")[\"y\"]\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        if len(y_train_full) < WINDOW_P95:\n",
    "            continue\n",
    "\n",
    "        p95 = np.percentile(y_train_full[-WINDOW_P95:], 95)\n",
    "\n",
    "        # ---- MASE denominator (computed once per uid+cutoff)\n",
    "        mase_denom_uid = mase_denom(y_train_full, m=MASE_M)\n",
    "\n",
    "        y_full = df_fold[\"y\"].values\n",
    "        y_base_full = df_fold[baseline_name].values\n",
    "\n",
    "        for model in model_cols:\n",
    "\n",
    "            yhat_full = df_fold[model].values\n",
    "\n",
    "            mask_valid = ~np.isnan(yhat_full)\n",
    "            if mask_valid.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            y_valid = y_full[mask_valid]\n",
    "            yhat = yhat_full[mask_valid]\n",
    "            y_base_valid = y_base_full[mask_valid]\n",
    "\n",
    "            # ================= RQ2 =================\n",
    "            mae_model = mae(y_valid, yhat)\n",
    "            rmse_model = rmse(y_valid, yhat)\n",
    "            smape_model = smape(y_valid, yhat)\n",
    "            mae_p95 = mae_conditional(y_valid, yhat, p95)\n",
    "\n",
    "            mae_base = mae(y_valid, y_base_valid)\n",
    "            rmse_base = rmse(y_valid, y_base_valid)\n",
    "            smape_base = smape(y_valid, y_base_valid)\n",
    "            mae_base_p95 = mae_conditional(y_valid, y_base_valid, p95)\n",
    "\n",
    "            skill_mae = skill_score(mae_model, mae_base)\n",
    "            skill_rmse = skill_score(rmse_model, rmse_base)\n",
    "            skill_smape = skill_score(smape_model, smape_base)\n",
    "            skill_p95 = skill_score(mae_p95, mae_base_p95)\n",
    "\n",
    "            # ---- MASE + Skill(MASE) vs baseline\n",
    "            mase_model = np.nan if (np.isnan(mase_denom_uid) or mase_denom_uid == 0) else mae_model / mase_denom_uid\n",
    "            mase_base = np.nan if (np.isnan(mase_denom_uid) or mase_denom_uid == 0) else mae_base / mase_denom_uid\n",
    "            skill_mase = skill_score(mase_model, mase_base)\n",
    "\n",
    "            bias_p95 = bias_conditional(y_valid, yhat, p95)\n",
    "            precision, recall, f1 = extreme_event_metrics(y_valid, yhat, p95)\n",
    "\n",
    "            # ================= RQ1 =================\n",
    "            model_rq1_col = model + \"_RQ1\"\n",
    "\n",
    "            if model_rq1_col in df_fold.columns:\n",
    "                yhat_rq1 = df_fold[model_rq1_col].values\n",
    "                mask_rq1 = ~np.isnan(yhat_rq1)\n",
    "                mask_joint = mask_valid & mask_rq1\n",
    "\n",
    "                if mask_joint.sum() > 0:\n",
    "                    y_joint = y_full[mask_joint]\n",
    "                    yhat_rq1_joint = yhat_rq1[mask_joint]\n",
    "\n",
    "                    mae_rq1 = mae(y_joint, yhat_rq1_joint)\n",
    "                    rmse_rq1 = rmse(y_joint, yhat_rq1_joint)\n",
    "                    smape_rq1 = smape(y_joint, yhat_rq1_joint)\n",
    "                    mae_p95_rq1 = mae_conditional(y_joint, yhat_rq1_joint, p95)\n",
    "\n",
    "                    skill_mae_vs_rq1 = skill_score(mae_model, mae_rq1)\n",
    "                    skill_rmse_vs_rq1 = skill_score(rmse_model, rmse_rq1)\n",
    "                    skill_smape_vs_rq1 = skill_score(smape_model, smape_rq1)\n",
    "                    skill_p95_vs_rq1 = skill_score(mae_p95, mae_p95_rq1)\n",
    "\n",
    "                    # ---- MASE vs RQ1 (same denominator, since y_train_full is fixed)\n",
    "                    mase_rq1 = np.nan if (np.isnan(mase_denom_uid) or mase_denom_uid == 0) else mae_rq1 / mase_denom_uid\n",
    "                    skill_mase_vs_rq1 = skill_score(mase_model, mase_rq1)\n",
    "                else:\n",
    "                    mae_rq1 = rmse_rq1 = smape_rq1 = mae_p95_rq1 = np.nan\n",
    "                    skill_mae_vs_rq1 = skill_rmse_vs_rq1 = skill_smape_vs_rq1 = skill_p95_vs_rq1 = np.nan\n",
    "                    mase_rq1 = skill_mase_vs_rq1 = np.nan\n",
    "            else:\n",
    "                mae_rq1 = rmse_rq1 = smape_rq1 = mae_p95_rq1 = np.nan\n",
    "                skill_mae_vs_rq1 = skill_rmse_vs_rq1 = skill_smape_vs_rq1 = skill_p95_vs_rq1 = np.nan\n",
    "                mase_rq1 = skill_mase_vs_rq1 = np.nan\n",
    "\n",
    "            # ================= SAVE =================\n",
    "            records.append({\n",
    "                \"pollutant\": pollutant,\n",
    "                \"fold\": fold,\n",
    "                \"horizon\": horizon,\n",
    "                \"unique_id\": uid,\n",
    "                \"cutoff\": cutoff,\n",
    "                \"model\": model,\n",
    "\n",
    "                \"MAE\": mae_model,\n",
    "                \"RMSE\": rmse_model,\n",
    "                \"sMAPE\": smape_model,\n",
    "                \"MAE_p95\": mae_p95,\n",
    "\n",
    "                \"Skill_MAE\": skill_mae,\n",
    "                \"Skill_RMSE\": skill_rmse,\n",
    "                \"Skill_sMAPE\": skill_smape,\n",
    "                \"Skill_p95\": skill_p95,\n",
    "\n",
    "                \"MASE\": mase_model,\n",
    "                \"MASE_base\": mase_base,\n",
    "                \"Skill_MASE\": skill_mase,\n",
    "                \"MASE_denom\": mase_denom_uid,\n",
    "                \"MASE_m\": MASE_M,\n",
    "\n",
    "                \"Bias_p95\": bias_p95,\n",
    "                \"Precision_p95\": precision,\n",
    "                \"Recall_p95\": recall,\n",
    "                \"F1_p95\": f1,\n",
    "\n",
    "                \"MAE_RQ1\": mae_rq1,\n",
    "                \"RMSE_RQ1\": rmse_rq1,\n",
    "                \"sMAPE_RQ1\": smape_rq1,\n",
    "                \"MAE_p95_RQ1\": mae_p95_rq1,\n",
    "\n",
    "                \"Skill_MAE_vs_RQ1\": skill_mae_vs_rq1,\n",
    "                \"Skill_RMSE_vs_RQ1\": skill_rmse_vs_rq1,\n",
    "                \"Skill_sMAPE_vs_RQ1\": skill_smape_vs_rq1,\n",
    "                \"Skill_p95_vs_RQ1\": skill_p95_vs_rq1,\n",
    "\n",
    "                \"MASE_RQ1\": mase_rq1,\n",
    "                \"Skill_MASE_vs_RQ1\": skill_mase_vs_rq1,\n",
    "            })\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# SAVE\n",
    "# ==================================================\n",
    "\n",
    "metrics_df = pd.DataFrame(records)\n",
    "metrics_df.to_csv(\"Results/RQ2/metrics.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
