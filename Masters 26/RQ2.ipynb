{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c973a4de",
   "metadata": {},
   "source": [
    "## **RQ2**\n",
    "\n",
    "> _Um modelo treinado em m√∫ltiplas c√©lulas espaciais generaliza melhor para c√©lulas n√£o vistas do que modelos treinados localmente?_\n",
    "\n",
    "Para garantir rigor metodol√≥gico e evitar vazamentos espaciais, a generaliza√ß√£o ser√° avaliada por protocolos explicitamente blindados, incluindo:\n",
    "* Split por blocos espaciais, evitando treino e teste em c√©lulas geograficamente adjacentes;\n",
    "* Dois cen√°rios complementares:\n",
    "    * Leave-region-out: regi√µes inteiras s√£o exclu√≠das do treino e usadas apenas para teste;\n",
    "    * Leave-cell-out: c√©lulas individuais n√£o vistas s√£o usadas para teste, respeitando separa√ß√£o espacial m√≠nima.\n",
    "* Esses protocolos permitem distinguir entre interpola√ß√£o espacial e generaliza√ß√£o genu√≠na."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b7db2",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ba11a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'\n",
    "\n",
    "import optuna\n",
    "import itertools\n",
    "import shutil\n",
    "import time\n",
    "import functools\n",
    "import gc\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.subplots\n",
    "import plotly.io as pio\n",
    "from graphmodex import plotlymodex\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab499e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "üí° Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n"
     ]
    }
   ],
   "source": [
    "import neuralforecast\n",
    "import mlforecast\n",
    "import statsforecast\n",
    "import utilsforecast\n",
    "import coreforecast\n",
    "\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import (\n",
    "    Naive, SeasonalNaive, \n",
    "    AutoARIMA, AutoCES, AutoETS, AutoTheta,\n",
    ")\n",
    "\n",
    "from mlforecast import MLForecast\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mlforecast.lag_transforms import ExpandingMean, RollingMean\n",
    "from mlforecast.target_transforms import Differences\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import (\n",
    "    NBEATS, NHITS,\n",
    "    GRU, Informer, LSTM\n",
    ")\n",
    "from neuralforecast.losses.pytorch import MSE, SMAPE, MAE\n",
    "\n",
    "from mlforecast.utils import PredictionIntervals\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(\n",
    "    max_steps=4,\n",
    "    logger=False,\n",
    "    enable_progress_bar=False,\n",
    "    enable_model_summary=False\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# REPRODUCTIBILITY\n",
    "# ==================================================\n",
    "import random\n",
    "import torch\n",
    "\n",
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1ce0c",
   "metadata": {},
   "source": [
    "### Results Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d0e9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_RESULTS = Path(\"Results/RQ2\")\n",
    "BASE_RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_results(df, model_family, pollutant, horizon_label, fold=None):\n",
    "\n",
    "    if fold is not None:\n",
    "        filename = f\"{model_family}_{pollutant}_{fold}_{horizon_label}.parquet\"\n",
    "    else:\n",
    "        filename = f\"{model_family}_{pollutant}_{horizon_label}.parquet\"\n",
    "\n",
    "\n",
    "    out_dir = BASE_RESULTS / model_family\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fname = f\"{pollutant}_{horizon_label}.csv\"\n",
    "    df.to_csv(out_dir / fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05ef21",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8408a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# DATA\n",
    "# ===============================\n",
    "df = pd.read_parquet(r'..\\Data\\CAMS\\processed\\eac4_era5_2010_2024_brasil_enhanced.parquet')\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# COASTAL ADJUSTMENT\n",
    "# ===============================\n",
    "cells = df[['unique_id', 'latitude', 'longitude', 'is_coastal']].drop_duplicates()\n",
    "\n",
    "coastal_cells = cells[cells['is_coastal'] == True]\n",
    "interior_cells = cells[cells['is_coastal'] == False]\n",
    "\n",
    "def min_distance_to_coast(row, coastal_coords):\n",
    "    dists = np.sqrt(\n",
    "        (coastal_coords[:,0] - row['latitude'])**2 +\n",
    "        (coastal_coords[:,1] - row['longitude'])**2\n",
    "    )\n",
    "    return dists.min()\n",
    "\n",
    "coastal_coords = coastal_cells[['latitude','longitude']].values\n",
    "\n",
    "interior_cells['dist_to_coast'] = interior_cells.apply(\n",
    "    lambda row: min_distance_to_coast(row, coastal_coords),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "buffer_threshold = 1.5\n",
    "\n",
    "deep_interior = interior_cells[\n",
    "    interior_cells['dist_to_coast'] > buffer_threshold\n",
    "]\n",
    "\n",
    "train_ids = deep_interior['unique_id']\n",
    "test_ids = coastal_cells['unique_id']\n",
    "\n",
    "df['train_coastal'] = df['unique_id'].isin(train_ids).astype(int)\n",
    "df['test_coastal']  = df['unique_id'].isin(test_ids).astype(int)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# POLLUTANTS\n",
    "# ===============================\n",
    "# pm10 = (\n",
    "#     df\n",
    "#     .copy()\n",
    "#     .rename(columns={\n",
    "#         'pm10': 'y',\n",
    "#         'valid_time': 'ds'        \n",
    "#     })\n",
    "#     [['unique_id', 'ds', 'y', \n",
    "#       'latitude', 'longitude', 'state', \n",
    "#       'name_region', 'test_coastal', \n",
    "#       'train_coastal',]]\n",
    "# )\n",
    "\n",
    "pm2p5 = (\n",
    "    df\n",
    "    .copy()\n",
    "    .rename(columns={\n",
    "        'pm2p5': 'y',\n",
    "        'valid_time': 'ds'        \n",
    "    })\n",
    "    [['unique_id', 'ds', 'y', \n",
    "      'latitude', 'longitude', 'state', \n",
    "      'name_region', 'test_coastal', \n",
    "      'train_coastal',]]\n",
    ")\n",
    "\n",
    "go3 = (\n",
    "    df\n",
    "    .copy()\n",
    "    .rename(columns={\n",
    "        'go3': 'y',\n",
    "        'valid_time': 'ds'        \n",
    "    })\n",
    "    [['unique_id', 'ds', 'y', \n",
    "      'latitude', 'longitude', 'state', \n",
    "      'name_region', 'test_coastal', \n",
    "      'train_coastal',]]\n",
    ")\n",
    "\n",
    "# no2 = (\n",
    "#     df\n",
    "#     .copy()\n",
    "#     .rename(columns={\n",
    "#         'no2': 'y',\n",
    "#         'valid_time': 'ds'        \n",
    "#     })\n",
    "#     [['unique_id', 'ds', 'y', \n",
    "#       'latitude', 'longitude', 'state', \n",
    "#       'name_region', 'test_coastal', \n",
    "#       'train_coastal',]]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f49b5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "pollutant_dict = {\n",
    "    'go3': {\n",
    "        'sudeste': {\n",
    "            'train_df': go3.query(\"name_region != 'Sudeste' and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "            'test_df': go3.query(\"name_region == 'Sudeste'\")[['unique_id', 'ds', 'y']],\n",
    "        },\n",
    "        'sul': {\n",
    "            'train_df': go3.query(\"name_region != 'Sul' and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "            'test_df': go3.query(\"name_region == 'Sul'\")[['unique_id', 'ds', 'y']],\n",
    "        },\n",
    "        'centro_oeste': {\n",
    "            'train_df': go3.query(\"name_region != 'Centro Oeste' and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "            'test_df': go3.query(\"name_region == 'Centro Oeste'\")[['unique_id', 'ds', 'y']],\n",
    "        },\n",
    "        'coastal': {\n",
    "            'train_df': go3.query(\"train_coastal == 1 and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "            'test_df': go3.query(\"test_coastal == 1\")[['unique_id', 'ds', 'y']],\n",
    "        },\n",
    "        'scaler': 1e8,\n",
    "    },\n",
    "    # 'no2': {\n",
    "    #     'sudeste': {\n",
    "    #         'train_df': no2.query(\"name_region != 'Sudeste' and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "    #         'test_df': no2.query(\"name_region == 'Sudeste'\")[['unique_id', 'ds', 'y']],\n",
    "    #     },\n",
    "    #     'sul': {\n",
    "    #         'train_df': no2.query(\"name_region != 'Sul' and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "    #         'test_df': no2.query(\"name_region == 'Sul'\")[['unique_id', 'ds', 'y']],\n",
    "    #     },\n",
    "    #     'centro_oeste': {\n",
    "    #         'train_df': no2.query(\"name_region != 'Centro Oeste' and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "    #         'test_df': no2.query(\"name_region == 'Centro Oeste'\")[['unique_id', 'ds', 'y']],\n",
    "    #     },\n",
    "    #     'coastal': {\n",
    "    #         'train_df': no2.query(\"train_coastal == 1 and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "    #         'test_df': no2.query(\"test_coastal == 1\")[['unique_id', 'ds', 'y']],\n",
    "    #     },\n",
    "    #     'scaler': 1e10,\n",
    "    # },\n",
    "    # 'pm10': {\n",
    "    #     'sudeste': {\n",
    "    #         'train_df': pm10.query(\"name_region != 'Sudeste' and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "    #         'test_df': pm10.query(\"name_region == 'Sudeste'\")[['unique_id', 'ds', 'y']],\n",
    "    #     },\n",
    "    #     'sul': {\n",
    "    #         'train_df': pm10.query(\"name_region != 'Sul' and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "    #         'test_df': pm10.query(\"name_region == 'Sul'\")[['unique_id', 'ds', 'y']],\n",
    "    #     },\n",
    "    #     'centro_oeste': {\n",
    "    #         'train_df': pm10.query(\"name_region != 'Centro Oeste' and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "    #         'test_df': pm10.query(\"name_region == 'Centro Oeste'\")[['unique_id', 'ds', 'y']],\n",
    "    #     },\n",
    "    #     'coastal': {\n",
    "    #         'train_df': pm10.query(\"train_coastal == 1 and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "    #         'test_df': pm10.query(\"test_coastal == 1\")[['unique_id', 'ds', 'y']],\n",
    "    #     },\n",
    "    #     'scaler': 1e9,\n",
    "    # },\n",
    "    'pm2p5': {\n",
    "        'sudeste': {\n",
    "            'train_df': pm2p5.query(\"name_region != 'Sudeste' and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "            'test_df': pm2p5.query(\"name_region == 'Sudeste'\")[['unique_id', 'ds', 'y']],\n",
    "        },\n",
    "        'sul': {\n",
    "            'train_df': pm2p5.query(\"name_region != 'Sul' and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "            'test_df': pm2p5.query(\"name_region == 'Sul'\")[['unique_id', 'ds', 'y']],\n",
    "        },\n",
    "        'centro_oeste': {\n",
    "            'train_df': pm2p5.query(\"name_region != 'Centro Oeste' and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "            'test_df': pm2p5.query(\"name_region == 'Centro Oeste'\")[['unique_id', 'ds', 'y']],\n",
    "        },\n",
    "        'coastal': {\n",
    "            'train_df': pm2p5.query(\"train_coastal == 1 and ds <= '2022-12-31'\")[['unique_id', 'ds', 'y']],\n",
    "            'test_df': pm2p5.query(\"test_coastal == 1\")[['unique_id', 'ds', 'y']],\n",
    "        },\n",
    "        'scaler': 1e9,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1c5ea",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a26aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CONFIG\n",
    "# ===============================\n",
    "steps_per_day = 8\n",
    "two_year_steps = 2 * 365 * steps_per_day\n",
    "target_windows = 30\n",
    "\n",
    "FREQ = '3h'\n",
    "SEASON_LENGTH = 8 \n",
    "\n",
    "experiments_dict = {\n",
    "    # '1 days': {\n",
    "    #     'horizon': 8*1,\n",
    "    #     'step_size': max(8*1, two_year_steps // target_windows),\n",
    "    #     'windows': target_windows,\n",
    "    # },\n",
    "    '7 days': {\n",
    "        'horizon': 8*7,\n",
    "        'step_size': max(8*7, two_year_steps // target_windows),\n",
    "        'windows': target_windows,\n",
    "    },\n",
    "    # '14 days': {\n",
    "    #     'horizon': 8*14,\n",
    "    #     'step_size': max(8*14, two_year_steps // target_windows),\n",
    "    #     'windows': target_windows,\n",
    "    # },\n",
    "    '30 days': {\n",
    "        'horizon': 8*30,\n",
    "        'step_size': 8*30,\n",
    "        'windows': two_year_steps // (8*30),  # 24\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e2b15",
   "metadata": {},
   "source": [
    "# **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2785802d",
   "metadata": {},
   "source": [
    "### Neural Forecasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollutant_name, pollutant_dict in pollutant_dict.items():\n",
    "\n",
    "    scaler = pollutant_dict['scaler']\n",
    "\n",
    "    for fold_name, fold_data in pollutant_dict.items():\n",
    "\n",
    "        if fold_name == 'scaler':\n",
    "            continue\n",
    "\n",
    "        train_df = fold_data['train_df'].copy()\n",
    "        test_df  = fold_data['test_df'].copy()\n",
    "\n",
    "        # Escalar\n",
    "        train_df['y'] *= scaler\n",
    "        test_df['y']  *= scaler\n",
    "\n",
    "        # Ordenar\n",
    "        train_df = train_df.sort_values(['unique_id','ds'])\n",
    "        test_df  = test_df.sort_values(['unique_id','ds'])\n",
    "\n",
    "        for horizon_label, exp_cfg in experiments_dict.items():\n",
    "\n",
    "            try:\n",
    "\n",
    "                print(f\"Running RQ2 | {pollutant_name} | {fold_name} | {horizon_label}\")\n",
    "\n",
    "                # -------------------------------\n",
    "                # MODELS\n",
    "                # -------------------------------\n",
    "                models = [\n",
    "                    NBEATS(\n",
    "                        h=exp_cfg['horizon'],\n",
    "                        input_size=14*8,\n",
    "                        stack_types=[\"identity\", \"trend\", \"seasonality\"],\n",
    "                        n_blocks=[1, 1, 1],\n",
    "                        mlp_units=3 * [[256, 256]],\n",
    "                        basis='polynomial',\n",
    "                        n_basis=2,\n",
    "                        n_harmonics=2,\n",
    "                        shared_weights=True,\n",
    "                        activation='ReLU',\n",
    "                        max_steps=1000,\n",
    "                        learning_rate=1e-3,\n",
    "                        batch_size=32,\n",
    "                        windows_batch_size=1024,\n",
    "                        random_seed=SEED,\n",
    "                        alias='NBEATS-I',\n",
    "                        loss=MAE(),\n",
    "                    ),\n",
    "                    NBEATS(\n",
    "                        h=exp_cfg['horizon'],\n",
    "                        input_size=14*8,\n",
    "                        stack_types=['identity'] * 3,\n",
    "                        n_blocks=[2, 2, 2],\n",
    "                        mlp_units=3 * [[256, 256]],\n",
    "                        shared_weights=False,\n",
    "                        activation='ReLU',\n",
    "                        max_steps=1000,\n",
    "                        learning_rate=1e-3,\n",
    "                        batch_size=32,\n",
    "                        windows_batch_size=1024,\n",
    "                        random_seed=SEED,\n",
    "                        alias='NBEATS-G',\n",
    "                        logger=False,\n",
    "                        loss=MAE(),\n",
    "                    ),\n",
    "                    NHITS(\n",
    "                        h=exp_cfg['horizon'],\n",
    "                        input_size=14*8,\n",
    "                        n_blocks=[1, 1, 1],\n",
    "                        mlp_units=3 * [[256, 256]],\n",
    "                        n_pool_kernel_size=[2, 2, 1],\n",
    "                        n_freq_downsample=[4, 2, 1],\n",
    "                        activation='ReLU',\n",
    "                        dropout_prob_theta=0.1,\n",
    "                        max_steps=1000,\n",
    "                        learning_rate=1e-3,\n",
    "                        batch_size=32,\n",
    "                        windows_batch_size=1024,\n",
    "                        random_seed=SEED,\n",
    "                        alias='NHITS',\n",
    "                        logger=False,\n",
    "                        loss=MAE(),\n",
    "                    ),\n",
    "                ]\n",
    "\n",
    "                nf = NeuralForecast(models=models, freq=FREQ)\n",
    "\n",
    "                # -------------------------\n",
    "                # FIT (APENAS TREINO)\n",
    "                # -------------------------\n",
    "                nf.fit(train_df)\n",
    "\n",
    "                # -------------------------\n",
    "                # GERAR CUTOFFS\n",
    "                # -------------------------\n",
    "                h = exp_cfg['horizon']\n",
    "                step_size = exp_cfg['step_size']\n",
    "                n_windows = exp_cfg['windows']\n",
    "\n",
    "                start_test = pd.Timestamp('2023-01-01')\n",
    "                end_test   = test_df['ds'].max()\n",
    "\n",
    "                rq1_cutoffs = pd.read_csv(\n",
    "                    rf'.\\Results\\RQ1\\ml\\go3_{horizon_label.replace(' ', '')}.csv'\n",
    "                )['cutoff'].unique()\n",
    "\n",
    "                rq1_cutoffs = pd.to_datetime(rq1_cutoffs)\n",
    "                rq1_cutoffs = sorted(rq1_cutoffs)\n",
    "\n",
    "                cutoffs = rq1_cutoffs[:n_windows]\n",
    "\n",
    "                results_list = []\n",
    "\n",
    "                # -------------------------\n",
    "                # LOOP CUTOFFS\n",
    "                # -------------------------\n",
    "                for cutoff in cutoffs:\n",
    "\n",
    "                    hist_test = test_df[test_df['ds'] <= cutoff]\n",
    "\n",
    "                    preds = nf.predict(\n",
    "                        h=h,\n",
    "                        df=hist_test\n",
    "                    )\n",
    "\n",
    "                    freq_offset = pd.tseries.frequencies.to_offset(FREQ)\n",
    "\n",
    "                    start_forecast = cutoff + freq_offset\n",
    "                    end_forecast = cutoff + h * freq_offset   # <-- CORRETO\n",
    "\n",
    "                    preds = preds[\n",
    "                        (preds['ds'] >= start_forecast) &\n",
    "                        (preds['ds'] <= end_forecast)\n",
    "                    ]\n",
    "\n",
    "                    real = test_df[\n",
    "                        (test_df['ds'] >= start_forecast) &\n",
    "                        (test_df['ds'] <= end_forecast)\n",
    "                    ]\n",
    "\n",
    "                    merged = preds.merge(real, on=['unique_id','ds'], how='left')\n",
    "                    merged['cutoff'] = cutoff\n",
    "                    merged['fold'] = fold_name\n",
    "\n",
    "                    results_list.append(merged)\n",
    "\n",
    "                results_ = pd.concat(results_list)\n",
    "\n",
    "                # ---------------------------------------\n",
    "                # Reescalar automaticamente todos modelos\n",
    "                # ---------------------------------------\n",
    "                non_model_cols = ['unique_id', 'ds', 'y', 'cutoff', 'fold']\n",
    "                model_cols = [col for col in results_.columns if col not in non_model_cols]\n",
    "                results_['y'] /= scaler\n",
    "\n",
    "                for col in model_cols:\n",
    "                    results_[col] /= scaler\n",
    "\n",
    "                save_results(\n",
    "                    results_,\n",
    "                    model_family=f\"neural\",\n",
    "                    pollutant=pollutant_name + f'_{fold_name}',\n",
    "                    horizon_label=horizon_label.replace(\" \",\"\"),\n",
    "                    fold=fold_name\n",
    "                )\n",
    "\n",
    "                clear_output(wait = True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro em {pollutant_name} | {fold_name} | {horizon_label}\")\n",
    "                print(e)\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170ee67",
   "metadata": {},
   "source": [
    "### Deep Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollutant_name, pollutant_dict in pollutant_dict.items():\n",
    "\n",
    "    scaler = pollutant_dict['scaler']\n",
    "\n",
    "    for fold_name, fold_data in pollutant_dict.items():\n",
    "\n",
    "        if fold_name == 'scaler':\n",
    "            continue\n",
    "\n",
    "        train_df = fold_data['train_df'].copy()\n",
    "        test_df  = fold_data['test_df'].copy()\n",
    "\n",
    "        # Escalar\n",
    "        train_df['y'] *= scaler\n",
    "        test_df['y']  *= scaler\n",
    "\n",
    "        # Ordenar\n",
    "        train_df = train_df.sort_values(['unique_id','ds'])\n",
    "        test_df  = test_df.sort_values(['unique_id','ds'])\n",
    "\n",
    "        for horizon_label, exp_cfg in experiments_dict.items():\n",
    "\n",
    "            try:\n",
    "\n",
    "                print(f\"Running RQ2 | {pollutant_name} | {fold_name} | {horizon_label}\")\n",
    "\n",
    "                # -------------------------------\n",
    "                # MODELS\n",
    "                # -------------------------------\n",
    "                models = [\n",
    "                    LSTM(\n",
    "                        h=exp_cfg['horizon'],\n",
    "                        input_size=14*8,\n",
    "                        max_steps=1000,  # Passos m√°ximos\n",
    "                        learning_rate=1e-3,  # Taxa de aprendizado\n",
    "                        batch_size=32,\n",
    "                        windows_batch_size=1024,\n",
    "                        random_seed=SEED,\n",
    "                        alias='LSTM',  # Alias para identifica√ß√£o\n",
    "                        loss=MAE(),  # Fun√ß√£o de perda\n",
    "                        logger=False,\n",
    "                    ),\n",
    "                    GRU(\n",
    "                        h=exp_cfg['horizon'],\n",
    "                        input_size=14*8,\n",
    "                        max_steps=1000,  # Passos m√°ximos\n",
    "                        learning_rate=1e-3,  # Taxa de aprendizado\n",
    "                        batch_size=32,\n",
    "                        windows_batch_size=1024,\n",
    "                        random_seed=SEED,\n",
    "                        alias='GRU',  # Alias para identifica√ß√£o\n",
    "                        loss=MAE(),  # Fun√ß√£o de perda\n",
    "                        logger=False,\n",
    "                    ),\n",
    "                    # Informer(\n",
    "                    #     h=exp_cfg['horizon'],\n",
    "                    #     input_size=14*8,\n",
    "                    #     max_steps=1500,  # Passos m√°ximos\n",
    "                    #     learning_rate=1e-3,  # Taxa de aprendizado\n",
    "                    #     batch_size=32,\n",
    "                    #     windows_batch_size=1024,\n",
    "                    #     random_seed=SEED,\n",
    "                    #     alias='Informer',\n",
    "                    #     loss=MAE(),  # Fun√ß√£o de perda\n",
    "                    #     logger=False,\n",
    "                    # )\n",
    "                ]\n",
    "\n",
    "                nf = NeuralForecast(models=models, freq=FREQ)\n",
    "\n",
    "                # -------------------------\n",
    "                # FIT (APENAS TREINO)\n",
    "                # -------------------------\n",
    "                nf.fit(train_df)\n",
    "\n",
    "                # -------------------------\n",
    "                # GERAR CUTOFFS\n",
    "                # -------------------------\n",
    "                h = exp_cfg['horizon']\n",
    "                step_size = exp_cfg['step_size']\n",
    "                n_windows = exp_cfg['windows']\n",
    "\n",
    "                start_test = pd.Timestamp('2023-01-01')\n",
    "                end_test   = test_df['ds'].max()\n",
    "\n",
    "                rq1_cutoffs = pd.read_csv(\n",
    "                    rf'.\\Results\\RQ1\\ml\\go3_{horizon_label.replace(' ', '')}.csv'\n",
    "                )['cutoff'].unique()\n",
    "\n",
    "                rq1_cutoffs = pd.to_datetime(rq1_cutoffs)\n",
    "                rq1_cutoffs = sorted(rq1_cutoffs)\n",
    "\n",
    "                cutoffs = rq1_cutoffs[:n_windows]\n",
    "\n",
    "                results_list = []\n",
    "\n",
    "                # -------------------------\n",
    "                # LOOP CUTOFFS\n",
    "                # -------------------------\n",
    "                for cutoff in cutoffs:\n",
    "\n",
    "                    hist_test = test_df[test_df['ds'] <= cutoff]\n",
    "\n",
    "                    preds = nf.predict(\n",
    "                        h=h,\n",
    "                        df=hist_test\n",
    "                    )\n",
    "\n",
    "                    freq_offset = pd.tseries.frequencies.to_offset(FREQ)\n",
    "\n",
    "                    start_forecast = cutoff + freq_offset\n",
    "                    end_forecast = cutoff + h * freq_offset   # <-- CORRETO\n",
    "\n",
    "                    preds = preds[\n",
    "                        (preds['ds'] >= start_forecast) &\n",
    "                        (preds['ds'] <= end_forecast)\n",
    "                    ]\n",
    "\n",
    "                    real = test_df[\n",
    "                        (test_df['ds'] >= start_forecast) &\n",
    "                        (test_df['ds'] <= end_forecast)\n",
    "                    ]\n",
    "\n",
    "                    merged = preds.merge(real, on=['unique_id','ds'], how='left')\n",
    "                    merged['cutoff'] = cutoff\n",
    "                    merged['fold'] = fold_name\n",
    "\n",
    "                    results_list.append(merged)\n",
    "\n",
    "                results_ = pd.concat(results_list)\n",
    "\n",
    "                # ---------------------------------------\n",
    "                # Reescalar automaticamente todos modelos\n",
    "                # ---------------------------------------\n",
    "                non_model_cols = ['unique_id', 'ds', 'y', 'cutoff', 'fold']\n",
    "                model_cols = [col for col in results_.columns if col not in non_model_cols]\n",
    "                results_['y'] /= scaler\n",
    "\n",
    "                for col in model_cols:\n",
    "                    results_[col] /= scaler\n",
    "\n",
    "                save_results(\n",
    "                    results_,\n",
    "                    model_family=f\"dl\",\n",
    "                    pollutant=pollutant_name + f'_{fold_name}',\n",
    "                    horizon_label=horizon_label.replace(\" \",\"\"),\n",
    "                    fold=fold_name\n",
    "                )\n",
    "\n",
    "                clear_output(wait = True)\n",
    "\n",
    "            except Exception as e:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596c623",
   "metadata": {},
   "source": [
    "### Machine Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f6936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RQ2 | go3 | centro_oeste | 7 days\n"
     ]
    }
   ],
   "source": [
    "for pollutant_name, pollutant_dict in pollutant_dict.items():\n",
    "\n",
    "    scaler = pollutant_dict['scaler']\n",
    "\n",
    "    for fold_name, fold_data in pollutant_dict.items():\n",
    "\n",
    "        if fold_name == 'scaler':\n",
    "            continue\n",
    "\n",
    "        train_df = fold_data['train_df'].copy()\n",
    "        test_df  = fold_data['test_df'].copy()\n",
    "\n",
    "        # Ordenar\n",
    "        train_df = train_df.sort_values(['unique_id','ds'])\n",
    "        test_df  = test_df.sort_values(['unique_id','ds'])\n",
    "\n",
    "        for horizon_label, exp_cfg in experiments_dict.items():\n",
    "            \n",
    "            if (\n",
    "                (pollutant_name == 'go3') and (fold_name == 'sudeste') \n",
    "            ) or (\n",
    "                (pollutant_name == 'go3') and (fold_name == 'sul') and (horizon_label == '7 days')\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            print(f\"Running RQ2 | {pollutant_name} | {fold_name} | {horizon_label}\")\n",
    "\n",
    "            # -------------------------------\n",
    "            # MODEL \n",
    "            # -------------------------------\n",
    "            lgb_model = lgb.LGBMRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                num_leaves=31,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=SEED,\n",
    "                n_jobs=3,\n",
    "            )\n",
    "            rf_model = RandomForestRegressor(\n",
    "                n_estimators=500,\n",
    "                max_depth=10,\n",
    "                random_state=SEED,\n",
    "                n_jobs=3,\n",
    "            )\n",
    "            xgb_model = xgb.XGBRegressor(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=SEED,\n",
    "                n_jobs=3,\n",
    "            )\n",
    "\n",
    "            # -------------------------------\n",
    "            # MLForecast (lags + calendar)\n",
    "            # -------------------------------\n",
    "            mlf = MLForecast(\n",
    "                models={\n",
    "                    'RandomForest': rf_model,\n",
    "                    'XGBoost': xgb_model,\n",
    "                    'LightGBM': lgb_model,\n",
    "                },\n",
    "                freq=FREQ,\n",
    "                lags=[1,2,4,8,16,24,56,112],\n",
    "                lag_transforms={\n",
    "                    8:  [RollingMean(8), ExpandingMean()],\n",
    "                    56: [RollingMean(56)],\n",
    "                },\n",
    "                date_features=['hour','dayofweek','month','dayofyear'],\n",
    "            )\n",
    "\n",
    "            # -------------------------\n",
    "            # FIT (APENAS TREINO)\n",
    "            # -------------------------\n",
    "            mlf.fit(train_df)\n",
    "\n",
    "            # -------------------------\n",
    "            # GERAR CUTOFFS\n",
    "            # -------------------------\n",
    "            h = exp_cfg['horizon']\n",
    "            step_size = exp_cfg['step_size']\n",
    "            n_windows = exp_cfg['windows']\n",
    "\n",
    "            start_test = pd.Timestamp('2023-01-01')\n",
    "            end_test   = test_df['ds'].max()\n",
    "\n",
    "            rq1_cutoffs = pd.read_csv(\n",
    "                rf'.\\Results\\RQ1\\ml\\go3_{horizon_label.replace(' ', '')}.csv'\n",
    "            )['cutoff'].unique()\n",
    "\n",
    "            rq1_cutoffs = pd.to_datetime(rq1_cutoffs)\n",
    "            rq1_cutoffs = sorted(rq1_cutoffs)\n",
    "\n",
    "            cutoffs = rq1_cutoffs[:n_windows]\n",
    "\n",
    "            results_list = []\n",
    "\n",
    "            # -------------------------\n",
    "            # LOOP CUTOFFS\n",
    "            # -------------------------\n",
    "            for cutoff in cutoffs:\n",
    "\n",
    "                hist_test = test_df[test_df['ds'] <= cutoff]\n",
    "\n",
    "                preds = mlf.predict(\n",
    "                    h=h,\n",
    "                    new_df=hist_test\n",
    "                )\n",
    "\n",
    "                freq_offset = pd.tseries.frequencies.to_offset(FREQ)\n",
    "\n",
    "                start_forecast = cutoff + freq_offset\n",
    "                end_forecast = cutoff + h * freq_offset   # <-- CORRETO\n",
    "\n",
    "                preds = preds[\n",
    "                    (preds['ds'] >= start_forecast) &\n",
    "                    (preds['ds'] <= end_forecast)\n",
    "                ]\n",
    "\n",
    "                real = test_df[\n",
    "                    (test_df['ds'] >= start_forecast) &\n",
    "                    (test_df['ds'] <= end_forecast)\n",
    "                ]\n",
    "\n",
    "                merged = preds.merge(real, on=['unique_id','ds'], how='left')\n",
    "                merged['cutoff'] = cutoff\n",
    "                merged['fold'] = fold_name\n",
    "\n",
    "                results_list.append(merged)\n",
    "\n",
    "            results_ml = pd.concat(results_list)\n",
    "\n",
    "            save_results(\n",
    "                results_ml,\n",
    "                model_family=f\"ml\",\n",
    "                pollutant=pollutant_name + f'_{fold_name}',\n",
    "                horizon_label=horizon_label.replace(\" \",\"\"),\n",
    "                fold=fold_name\n",
    "            )\n",
    "\n",
    "            clear_output(wait = True)\n",
    "            # deletar objetos grandes\n",
    "            del mlf\n",
    "            del lgb_model\n",
    "            del rf_model\n",
    "            del results_ml\n",
    "            del results_list\n",
    "\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae113ef9",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0145a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_RESULTS = Path(\"Results/RQ2\")\n",
    "FULL_DIR = BASE_RESULTS / \"full\"\n",
    "FULL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "families = [\"ml\", \"dl\", \"neural\"]\n",
    "\n",
    "def build_full_results(pollutant, fold, horizon_label):\n",
    "    \"\"\"\n",
    "    Concatena fam√≠lias horizontalmente via concat,\n",
    "    garantindo que n√£o haja perda de linhas.\n",
    "    \"\"\"\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for family in families:\n",
    "        fpath = BASE_RESULTS / family / f\"{pollutant}_{fold}_{horizon_label}.csv\"\n",
    "\n",
    "        if fpath.exists():\n",
    "            df = pd.read_csv(fpath)\n",
    "\n",
    "            if \"fit_time_seconds\" in df.columns:\n",
    "                df = df.drop(columns=[\"fit_time_seconds\"])\n",
    "\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Missing: {fpath}\")\n",
    "\n",
    "    if not dfs:\n",
    "        return None\n",
    "\n",
    "    # Ordenar todos igualmente\n",
    "    for i in range(len(dfs)):\n",
    "        dfs[i] = dfs[i].sort_values(\n",
    "            [\"unique_id\", \"ds\", \"cutoff\", \"fold\"]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    # Usar o primeiro como base\n",
    "    base = dfs[0][[\"unique_id\", \"ds\", \"cutoff\", \"y\", \"fold\"]].copy()\n",
    "\n",
    "    # Adicionar modelos das outras fam√≠lias\n",
    "    for df in dfs:\n",
    "\n",
    "        model_cols = [\n",
    "            c for c in df.columns\n",
    "            if c not in [\"unique_id\", \"ds\", \"cutoff\", \"y\", \"fold\"]\n",
    "        ]\n",
    "\n",
    "        base = pd.concat(\n",
    "            [base, df[model_cols]],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "for pollutant_name, pollutant_dict in pollutant_dict.items():\n",
    "\n",
    "    for fold_name in pollutant_dict.keys():\n",
    "\n",
    "        if fold_name == \"scaler\":\n",
    "            continue\n",
    "\n",
    "        for horizon_label in experiments_dict.keys():\n",
    "\n",
    "            horizon_clean = horizon_label.replace(\" \", \"\")\n",
    "\n",
    "            print(f\"Building FULL | {pollutant_name} | {fold_name} | {horizon_clean}\")\n",
    "\n",
    "            df_full = build_full_results(\n",
    "                pollutant=pollutant_name,\n",
    "                fold=fold_name,\n",
    "                horizon_label=horizon_clean\n",
    "            )\n",
    "\n",
    "            if df_full is None:\n",
    "                continue\n",
    "\n",
    "            df_full.to_csv(\n",
    "                FULL_DIR / f\"{pollutant_name}_{fold_name}_{horizon_clean}.csv\",\n",
    "                index=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bbb64f",
   "metadata": {},
   "source": [
    "## **Estat√≠sticas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a434eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# METRICS\n",
    "# ==================================================\n",
    "\n",
    "def mae(y, yhat):\n",
    "    return np.mean(np.abs(y - yhat))\n",
    "\n",
    "def mse(y, yhat):\n",
    "    return np.mean((y - yhat) ** 2)\n",
    "\n",
    "def rmse(y, yhat):\n",
    "    return np.sqrt(mse(y, yhat))\n",
    "\n",
    "def smape(y, yhat):\n",
    "    denom = (np.abs(y) + np.abs(yhat)) / 2\n",
    "    mask = denom != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y[mask] - yhat[mask]) / denom[mask])\n",
    "\n",
    "def mae_conditional(y, yhat, threshold):\n",
    "    mask = y >= threshold\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs(y[mask] - yhat[mask]))\n",
    "\n",
    "def bias_conditional(y, yhat, threshold):\n",
    "    mask = y >= threshold\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(yhat[mask] - y[mask])\n",
    "\n",
    "def skill_score(model_err, baseline_err):\n",
    "    if baseline_err == 0 or np.isnan(baseline_err):\n",
    "        return np.nan\n",
    "    return 1 - model_err / baseline_err\n",
    "\n",
    "def extreme_event_metrics(y, yhat, threshold):\n",
    "    y_event = y >= threshold\n",
    "    yhat_event = yhat >= threshold\n",
    "\n",
    "    tp = np.sum(y_event & yhat_event)\n",
    "    fp = np.sum(~y_event & yhat_event)\n",
    "    fn = np.sum(y_event & ~yhat_event)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "\n",
    "    if precision > 0 and recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        f1 = np.nan\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# CONFIG\n",
    "# ==================================================\n",
    "\n",
    "RQ2_BASE = Path(\"Results/RQ2/full\")\n",
    "RQ1_BASE = Path(\"Results/RQ1/full\")\n",
    "\n",
    "baseline_name = \"Naive\"\n",
    "WINDOW_P95 = 365 * 8  # 1 ano em 3h\n",
    "\n",
    "records = []\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# LOOP RQ2 FILES\n",
    "# ==================================================\n",
    "\n",
    "for file in RQ2_BASE.glob(\"*.csv\"):\n",
    "\n",
    "    # Ex: go3_sudeste_1days.csv\n",
    "    parts = file.stem.split(\"_\")\n",
    "\n",
    "    pollutant = parts[0]\n",
    "    horizon = parts[-1]\n",
    "    fold = \"_\".join(parts[1:-1])\n",
    "\n",
    "    print(f\"Evaluating | {pollutant} | {fold} | {horizon}\")\n",
    "\n",
    "    df_pred = pd.read_csv(file)\n",
    "    df_pred[\"ds\"] = pd.to_datetime(df_pred[\"ds\"])\n",
    "    df_pred[\"cutoff\"] = pd.to_datetime(df_pred[\"cutoff\"])\n",
    "\n",
    "    # --------------------------------------\n",
    "    # GROUND TRUTH (VINDO DO pollutant_dict)\n",
    "    # --------------------------------------\n",
    "    test_df = pollutant_dict[pollutant][fold][\"test_df\"].copy()\n",
    "    test_df[\"ds\"] = pd.to_datetime(test_df[\"ds\"])\n",
    "\n",
    "    # --------------------------------------\n",
    "    # LOAD RQ1 (PODE N√ÉO EXISTIR PARA ALGUNS IDs)\n",
    "    # --------------------------------------\n",
    "    rq1_path = RQ1_BASE / f\"{pollutant}_{horizon}.csv\"\n",
    "\n",
    "    if rq1_path.exists():\n",
    "        df_rq1 = pd.read_csv(rq1_path)\n",
    "        df_rq1[\"ds\"] = pd.to_datetime(df_rq1[\"ds\"])\n",
    "        df_rq1[\"cutoff\"] = pd.to_datetime(df_rq1[\"cutoff\"])\n",
    "\n",
    "        df_pred = df_pred.merge(\n",
    "            df_rq1,\n",
    "            on=[\"unique_id\", \"ds\", \"cutoff\"],\n",
    "            suffixes=(\"\", \"_RQ1\"),\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    # --------------------------------------\n",
    "    # MODELOS\n",
    "    # --------------------------------------\n",
    "    model_cols = [\n",
    "        c for c in df_pred.columns\n",
    "        if (\n",
    "            c not in [\"unique_id\", \"ds\", \"cutoff\", \"y\", \"fold\"]\n",
    "            and not c.endswith(\"_RQ1\")\n",
    "            and df_pred[c].dtype in [np.float64, np.float32]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # --------------------------------------\n",
    "    # LOOP POR UNIQUE_ID + CUTOFF\n",
    "    # --------------------------------------\n",
    "    for (uid, cutoff), df_fold in df_pred.groupby([\"unique_id\", \"cutoff\"]):\n",
    "\n",
    "        y_train_full = (\n",
    "            test_df\n",
    "            .query(\"unique_id == @uid and ds <= @cutoff\")[\"y\"]\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        if len(y_train_full) < WINDOW_P95:\n",
    "            continue\n",
    "\n",
    "        p95 = np.percentile(y_train_full[-WINDOW_P95:], 95)\n",
    "\n",
    "        y_full = df_fold[\"y\"].values\n",
    "        y_base_full = df_fold[baseline_name].values\n",
    "\n",
    "        for model in model_cols:\n",
    "\n",
    "            yhat_full = df_fold[model].values\n",
    "\n",
    "            mask_valid = ~np.isnan(yhat_full)\n",
    "            if mask_valid.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            y_valid = y_full[mask_valid]\n",
    "            yhat = yhat_full[mask_valid]\n",
    "            y_base_valid = y_base_full[mask_valid]\n",
    "\n",
    "            # ================= RQ2 =================\n",
    "            mae_model = mae(y_valid, yhat)\n",
    "            rmse_model = rmse(y_valid, yhat)\n",
    "            smape_model = smape(y_valid, yhat)\n",
    "            mae_p95 = mae_conditional(y_valid, yhat, p95)\n",
    "\n",
    "            mae_base = mae(y_valid, y_base_valid)\n",
    "            rmse_base = rmse(y_valid, y_base_valid)\n",
    "            smape_base = smape(y_valid, y_base_valid)\n",
    "            mae_base_p95 = mae_conditional(y_valid, y_base_valid, p95)\n",
    "\n",
    "            skill_mae = skill_score(mae_model, mae_base)\n",
    "            skill_rmse = skill_score(rmse_model, rmse_base)\n",
    "            skill_smape = skill_score(smape_model, smape_base)\n",
    "            skill_p95 = skill_score(mae_p95, mae_base_p95)\n",
    "\n",
    "            bias_p95 = bias_conditional(y_valid, yhat, p95)\n",
    "            precision, recall, f1 = extreme_event_metrics(y_valid, yhat, p95)\n",
    "\n",
    "            # ================= RQ1 =================\n",
    "            model_rq1_col = model + \"_RQ1\"\n",
    "\n",
    "            if model_rq1_col in df_fold.columns:\n",
    "                yhat_rq1 = df_fold[model_rq1_col].values\n",
    "                mask_rq1 = ~np.isnan(yhat_rq1)\n",
    "                mask_joint = mask_valid & mask_rq1\n",
    "\n",
    "                if mask_joint.sum() > 0:\n",
    "                    y_joint = y_full[mask_joint]\n",
    "                    yhat_rq1_joint = yhat_rq1[mask_joint]\n",
    "\n",
    "                    mae_rq1 = mae(y_joint, yhat_rq1_joint)\n",
    "                    rmse_rq1 = rmse(y_joint, yhat_rq1_joint)\n",
    "                    smape_rq1 = smape(y_joint, yhat_rq1_joint)\n",
    "                    mae_p95_rq1 = mae_conditional(y_joint, yhat_rq1_joint, p95)\n",
    "\n",
    "                    skill_mae_vs_rq1 = skill_score(mae_model, mae_rq1)\n",
    "                    skill_rmse_vs_rq1 = skill_score(rmse_model, rmse_rq1)\n",
    "                    skill_smape_vs_rq1 = skill_score(smape_model, smape_rq1)\n",
    "                    skill_p95_vs_rq1 = skill_score(mae_p95, mae_p95_rq1)\n",
    "                else:\n",
    "                    mae_rq1 = rmse_rq1 = smape_rq1 = mae_p95_rq1 = np.nan\n",
    "                    skill_mae_vs_rq1 = skill_rmse_vs_rq1 = skill_smape_vs_rq1 = skill_p95_vs_rq1 = np.nan\n",
    "            else:\n",
    "                mae_rq1 = rmse_rq1 = smape_rq1 = mae_p95_rq1 = np.nan\n",
    "                skill_mae_vs_rq1 = skill_rmse_vs_rq1 = skill_smape_vs_rq1 = skill_p95_vs_rq1 = np.nan\n",
    "\n",
    "            # ================= SAVE =================\n",
    "            records.append({\n",
    "                \"pollutant\": pollutant,\n",
    "                \"fold\": fold,\n",
    "                \"horizon\": horizon,\n",
    "                \"unique_id\": uid,\n",
    "                \"cutoff\": cutoff,\n",
    "                \"model\": model,\n",
    "\n",
    "                \"MAE\": mae_model,\n",
    "                \"RMSE\": rmse_model,\n",
    "                \"sMAPE\": smape_model,\n",
    "                \"MAE_p95\": mae_p95,\n",
    "\n",
    "                \"Skill_MAE\": skill_mae,\n",
    "                \"Skill_RMSE\": skill_rmse,\n",
    "                \"Skill_sMAPE\": skill_smape,\n",
    "                \"Skill_p95\": skill_p95,\n",
    "\n",
    "                \"Bias_p95\": bias_p95,\n",
    "                \"Precision_p95\": precision,\n",
    "                \"Recall_p95\": recall,\n",
    "                \"F1_p95\": f1,\n",
    "\n",
    "                \"MAE_RQ1\": mae_rq1,\n",
    "                \"RMSE_RQ1\": rmse_rq1,\n",
    "                \"sMAPE_RQ1\": smape_rq1,\n",
    "                \"MAE_p95_RQ1\": mae_p95_rq1,\n",
    "\n",
    "                \"Skill_MAE_vs_RQ1\": skill_mae_vs_rq1,\n",
    "                \"Skill_RMSE_vs_RQ1\": skill_rmse_vs_rq1,\n",
    "                \"Skill_sMAPE_vs_RQ1\": skill_smape_vs_rq1,\n",
    "                \"Skill_p95_vs_RQ1\": skill_p95_vs_rq1,\n",
    "            })\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# SAVE\n",
    "# ==================================================\n",
    "\n",
    "metrics_df = pd.DataFrame(records)\n",
    "metrics_df.to_csv(\"Results/RQ2/metrics.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
